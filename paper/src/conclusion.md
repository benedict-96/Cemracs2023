# Conclusion and future work

We have introduced a new neural network architecture, referred to as *volume-preserving transformer*, for learning the dynamics of systems described by divergence-free vector fields. This new architecture is based on the classical transformer, but modifies the attention mechanism, such that the resulting integrator is volume-preserving. We have shown that the new network leads to more accurate results than both the classical transformer and volume-preserving feedforward networds, when applied to volume-preserving dynamical systems, specifically the rigid body. 

Future work will focus on the application of the new architecture to different systems and in particular parameter-dependent equations. A more thorough study of why and when classical attention fails (as was demonstrated in this work) is also desirable. Another interesting objective for future research is the proof of an universal approximation theorem for the volume-preserving feedforward neural network and perhaps even the transformer presented in this work. As was already observed by other authors [jin2020sympnets, bajars2023locally](@cite), efforts should also be directed towards making the learning process more efficient by reducing the number of epochs for small networks by using e.g. a Newton optimizer instead of Adam. Lastly the poor performance of the volume-preserving feedforward neural network, when used has an integrator, has to be investigated further.

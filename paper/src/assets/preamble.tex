%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a template file for Proccedings 
%
% Copy it to a new file with a new name and use it as the basis
% for your article
%
%%%%%%%%%%%%%%%%%%%%%%%%   EDP Sciences  %%%%%%%%%%%%%%%%%%%%%%%%%%
%
\documentclass[proc]{edpsmath}


%%%%%%%%%%%%%--PREAMBLE--%%%%%%%%%%%%%%%%%%
\usepackage{./documenter_custom}
\usepackage{./custom}

%%%%%%%%%%%%%%%--BODY--%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Volume-Preserving Transformers for Learning Time Series Data with Structure}% \thanks{CIRM}% At most 5 thanks
%
\author{Benedikt Brantner}\address{Max-Planck-Institut f\"ur Plasmaphysik, Boltzmannstra\ss{}e 2, 85748 Garching}
\author{Michael Kraus}\sameaddress{1} %{Max-Planck-Institut f\"ur Plasmaphysik, Boltzmannstra\ss{}e 2, 85748 Garching}
\author{Zeyuan Li}\address{Zentrum Mathematik, Technische Universität München, Boltzmannstra\ss{}e 3, 85748 Garching, Germany}
\author{Guillaume de Romemont} \address{DAAA, ONERA, Université Paris Saclay, F-92322, Châtillon, France}\secondaddress{Arts et Métiers Institute of Technology, Paris, France}
%
\date{\today}

\begin{abstract} 
    Two of the many trends in neural network research of the past few years have been (i) the learning of dynamical systems, especially with recurrent neural networks such as long short-term memory networks (LSTMs) and (ii) the introduction of transformer neural networks for natural language processing (NLP) tasks. Both of these trends have created enormous amounts of traction, particularly the second one: transformer networks now dominate the field of NLP. 

    Even though some work has been performed on the intersection of these two trends, those efforts was largely limited to using the vanilla transformer directly without adjusting its architecture for the setting of a physical system.

    In this work we use a transformer-inspired neural network to learn a dynamical system and furthermore (for the first time) imbue it with structure-preserving properties to improve long-term stability. This is shown to be of great advantage when applying the neural network to real world applications.
\end{abstract}
%
\begin{resume} 
    Deux des nombreuses tendances de la recherche sur les réseaux neuronaux de ces dernières années ont été (i) l'apprentissage des systèmes dynamiques, en particulier avec les réseaux neuronaux récurrents tels que les Long Short Term Memory (LSTM) et (ii) l'introduction de réseaux neuronaux transformer pour le traitement du langage naturel (NLP). Ces deux tendances ont suscité un énorme engouement, en particulier la seconde : les réseaux transformer dominent désormais le NLP. 

    Bien que certains travaux aient été réalisés à l'intersection de ces deux tendances, ils se sont largement limités à l'utilisation directe du transformer vanilla sans adapter son architecture à la configuration d'un système physique.
    
    Dans ce cadre, nous utilisons un réseau neuronal inspiré d'un transformer pour apprendre un système dynamique et, en outre (pour la première fois), lui conférer des propriétés de préservation de la structure afin d'améliorer la stabilité à long terme. Ces propriétés s'avèrent extrêmement importantes lors de l'application du réseau neuronal à des applications réelles.
    
\end{resume}
%53Z50, 53C30, 68T07, 68W10, 90C26
\subjclass{ 68T07, % artifical neural networks and deep learning
            65D30, % numerical integration 
            37M15, % Discretization methods and integrators (symplectic, variational, geometric, etc.) for dynamical systems
            65P10  % Numerical methods for Hamiltonian systems including symplectic integrators
}
%
\keywords{Reduced-Order Modeling, Hyper Reduction, Machine Learning, Transformers, Neural Networks, Hamiltonian, Symplectic, Structure-Preserving, Multi-Step Methods}
%
%
\maketitle

Deux des nombreuses tendances de la recherche sur les réseaux neuronaux de ces dernières années ont été (i) l'apprentissage des systèmes dynamiques, en particulier avec les réseaux neuronaux récurrents tels que les Long Short Term Memory (LSTM) et (ii) l'introduction de réseaux neuronaux transformer pour le traitement du langage naturel (NLP). Ces deux tendances ont suscité un énorme engouement, en particulier la seconde : les réseaux transformer dominent désormais le NLP. 

Bien que certains travaux aient été réalisés à l'intersection de ces deux tendances, ils se sont largement limités à l'utilisation directe du transformer vanilla sans adapter son architecture à la configuration d'un système physique.
    
Dans ce cadre, nous utilisons un réseau neuronal inspiré d'un transformer pour apprendre un système dynamique et, en outre (pour la première fois), lui conférer des propriétés de préservation de la structure afin d'améliorer la stabilité à long terme. Ces propriétés s'avèrent extrêmement importantes lors de l'application du réseau neuronal à des applications réelles.
Deux des nombreuses tendances de la recherche sur les réseaux de neurones de ces dernières années ont été (i) l'apprentissage des systèmes dynamiques, en particulier avec les réseaux de neurones récurrents tels que les `Long Short Term Memory' (LSTM) et (ii) l'introduction de réseaux de neurones de type `transformers' pour le traitement du langage naturel (NLP). % Ces deux tendances ont suscité un énorme engouement, en particulier la seconde : les réseaux transformers dominent désormais le NLP.

Bien que certains travaux aient été réalisés à l'intersection de ces deux tendances, ils se sont largement limités à l'utilisation directe du transformer vanilla sans adapter son architecture à la configuration d'un système physique.

Dans ce travail, nous utilisons un réseau de neurones inspiré d'un transformer pour apprendre un système dynamique et, de plus (pour la première fois), nous changeons la fonction d'activation du niveau d'attention afin de lui conférer des propriétés de préservation de la structure dans le but d'améliorer sa stabilité à long terme. Ces propriétés s'avèrent extrêmement importantes lors de l'application du réseau de neurone à la trajectoir d'un corps rigide.